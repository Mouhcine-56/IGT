\begin{abstract}
This paper first introduces a method to approximate the value function of high-dimensional optimal control problems by neural networks.  Based on the established relationship between Pontryagin's maximum principle (PMP) and the value function, which is characterized as being the unique solution to an associated  Hamilton-Jacobi-Bellman (HJB) equation, we propose an approach that begins by using neural networks to provide a first rough estimate of the value function, which serves as initialization for solving the two point boundary value problem in the PMP and, as a result, generates reliable data. To train the neural network we define a loss function that  takes into account this dataset and also penalizes deviations from the HJB equation.

In the second part, we address the computation of equilibria in first-order Mean Field Game (MFG) problems by integrating our method with the fictitious play algorithm. These equilibria are characterized by a coupled system of a first-order HJB equation and a continuity equation. To approximate the solution to the continuity equation, we introduce a second neural network that learns the flow map transporting the initial distribution of agents. This network is trained on data generated by solving the underlying ODEs for a batch of initial conditions sampled from the initial distribution of agents. The iterative procedure is initialized by a joint training of the value function and the flow map, minimizing a composite loss function of HJB and ODE residuals. By combining this initialization, the flow approximation, the previously described method for approximating the value function, and the fictitious play algorithm, we obtain an effective method to tackle high-dimensional deterministic MFGs.
\end{abstract}

\section{Introduction}
\label{Sect_Intro} 



Machine learning techniques applied to optimal control theory have been an active research field over the last decade. Indeed, for reasons of robustness, a critical issue in optimal control theory is the approximation of optimal feedback controllers, and one of the main techniques to achieve this goal is to approximate the solution to the associated Hamilton--Jacobi--Bellman (HJB) equation~\cite{bardi1997optimal,falcone2013semi}. In the framework of finite horizon deterministic problems, the HJB equation is a first-order nonlinear PDE that describes the optimal cost in terms of the initial time and state, and whose numerical approximation by classical methods such as finite difference schemes, semi-Lagrangian schemes, and finite elements suffers from the so-called \emph{curse of dimensionality}~\cite{MR134403}, as these methods are based on spatial grid discretizations. We refer the reader to~\cite{falcone2013semi,MR3653852} and the references therein for an overview of solving HJB equations with grid-based methods. To mitigate the issues arising from high state dimensions, several approaches have been considered in recent years, including, but not limited to, tropical methods~\cite{MR2346381,MR2385864,MR2599910,MR4546175}, semi-Lagrangian schemes defined on sparse grids~\cite{MR3045704,MR3592131}, polynomial approximation~\cite{kang2017mitigating,MR3769705,MR4109008,MR4253741,MR4664738}, optimization methods based on the Hopf and Lax-Oleinik formulae~\cite{MR3413587,MR3543239,MR4221591,MR4754321,MR4717768,MR4713729}, semi-Lagrangian schemes using tree structures~\cite{MR3984311,MR4506570}, tensor decomposition techniques~\cite{Horowitz14,Gorodetsky_2018,MR4254983,MR4442445,MR4729053,MR4641650}, and neural network (NN) approximations~\cite{MR4123395,MR4081911,MR4338293,MR4233238,MR4166062,nakamura2021adaptive,MR4275044,onken2022neural,MR4646437,MR4907583,MR4793480,MR4834792}.

 

Having efficient methods at our disposal to solve high-dimensional HJB equations paves the way for tackling high-dimensional Mean Field Games (MFGs). These models, introduced independently by J.-M.~Lasry and P.-L.~Lions in~\cite{LasryLions06i,LasryLions06ii,MR2295621} and by Caines, Huang, and Malham\'e in~\cite{HMC06}, describe the asymptotic behavior of Nash equilibria of symmetric stochastic games as the number of players tends to infinity. We refer the reader to~\cite{MR3195844,MR3559742,MR3752669,MR3753660,MR4214773} for an overview of MFG theory, including their applications to crowd-motion models, economics, and finance. In their simplest form, MFG equilibria are characterized by a system of two PDEs: a HJB equation, which describes the value function of a typical player, and a transport equation, which describes the distribution of the agents. The numerical approximation of this PDE system has been tackled by grid-based methods such as finite difference schemes~\cite{AchdouCapuzzo10,MR3097034,MR3452251}, semi-Lagrangian schemes~\cite{MR3148086,CarliniSilva18,MR3392626,Silva2024LG}, finite elements discretizations~\cite{MR4688199,MR4858134,MR4854642},  approximation by finite-state discrete-time MFGs~\cite{MR4030259,MR4835150,MR4714572}, and, for high-dimensional problems, machine learning techniques~\cite{MR4264647,MR4236167,MR4522347,MR4587578,assouli2023deep,assouli2023policy}. We refer the reader to~\cite{MR4214777,MR4368188} for an overview of numerical aspects of MFGs, including variational techniques.

Our first contribution in this paper is the introduction of a method, called Initialization-Generation-Training (IGT), which, similarly to~\cite{kang2015causality,kang2017mitigating,nakamura2021adaptive,MR4253741,MR4442445}, builds an approximation of the value function from data generated by solving, in open-loop, a family of optimal control problems parameterized by the initial time and initial state. Each of these problems is tackled using Pontryagin's Maximum Principle (PMP)~\cite{MR166037}, which provides a necessary condition for local optimality taking the form of a two-point boundary value problem (TPBVP) for the optimal state and its adjoint state. A key distinction from previous works is the use of well-known sensitivity relations in optimal control theory, which imply that, if the value function is sufficiently smooth, the aforementioned TPBVP is also sufficient for {\it global} optimality as soon as the adjoint state trajectory matches the spatial gradient of the value function evaluated at the state trajectory (see Section~\ref{sec:preliminaries} below). As a consequence, to foster global optimality when solving the TPBVP, it is reasonable to incorporate this relation in the initialization step of an iterative method. Other approaches such as the Adaptive Sampling and Model Refinement (ASMR) algorithm, introduced in \cite{nakamura2021adaptive} and inspired by the method in \cite{MR4275044}, seek to reduce the sensitivity to the initial guess when solving the TPBVP by incorporating a {\it time-marching} technique with intermediate times. However, this technique has failed to converge in some of the examples treated in Section~\ref{sec:numerical_results} below. Specifically, in our implementation, we compute a first (rough) NN approximation of the value function with the Deep Galerkin Method (DGM)~\cite{sirignano2018dgm}, minimizing the residual of the HJB equation. The spatial gradient of this approximation is then used to provide a suitable initialization of Newton's iterates to approximate global solutions to the parameterized family of optimal control problems, generating reliable data including optimal costs, optimal states, and adjoint states. The resulting dataset is then employed to train a NN through the minimization of loss functions involving the generated data and the HJB equation, thus providing an improved approximation of the value function. The latter can be used to build approximate optimal feedback laws and can also serve as input to generate new data, thereby enhancing accuracy in a second round of the method.

In our second contribution, we combine the IGT method with a NN approximation of solutions to continuity equations to handle the approximation of first-order MFG systems. Indeed, when the underlying differential games are deterministic, the PDE system introduced in~\cite{MR2295621}, describing Nash equilibria of the game with a continuum of agents, is given by a first-order HJB equation coupled with a continuity equation. It follows from the \emph{fictitious play method}, introduced in the context of MFGs in~\cite{MR3608094,These_Saeed_18}, that one can approximate their solutions by solving both equations separately and iteratively. In turn, we can combine these iterates with the IGT method to approximate the HJB equations and with a NN method to approximate the solutions to the continuity equations. Since the solution to the continuity equation is given by the push-forward of the initial distribution of the agents through a flow that depends on the solution to the HJB equation, it is natural to approximate this flow by a NN depending on the current NN approximation of the value function. To this end, we first generate data by solving the underlying ODEs over a batch of initial conditions chosen randomly according to the initial distribution of the agents, and we use these data to train the NN approximation of the flow. The iterative procedure is initialized by a joint training of the value function and the flow map, minimizing a composite loss function of HJB and ODE residuals. To monitor convergence of the fictitious play algorithm, in our numerical tests we compute the aggregated Sinkhorn divergence between a distribution of agents and its best response at each iteration.  In addition, we also check the  exploitability as a complementary performance indicator.

The remainder of the paper is structured as follows. Section~\ref{sec:preliminaries} introduces and recalls some basic facts on the optimal control and MFG problems we are interested in. Section~\ref{sec:IGT} provides the details of the IGT method for finite horizon deterministic optimal control problems, while Section~\ref{sec:MFG_approximation} explains how we can combine this method with a NN approximation of solutions to continuity equations and fictitious play iterates to approximate solutions to MFG systems. Finally, Section~\ref{sec:numerical_results} first shows the performance of the IGT method in high-dimensional examples, including linear-quadratic problems, the optimal guidance of a quadcopter, and an optimal control problem with obstacle avoidance. Due to their complexity, the initialization step in the IGT method plays a crucial role in ensuring convergence in the last two examples. The second part of this section deals with the approximation of high-dimensional first-order MFGs. We show the performance of the method for a linear-quadratic MFG with an explicit solution, a MFG in which the agents control their acceleration~\cite{MR4102464,MR4132067,MR4177552}, and a MFG in which the agents avoid obstacles and have an aversion to crowded regions.

\section{Approximation of solutions to high-dimensional first-order mean field games systems}
\label{sec:MFG_approximation}
Given an initial guess $[0,T]\ni t\mapsto \overline{m}(t)\in\P(\RR^{d})$, which is initialized as described below{\color{blue} , the IGT method presented in the previous section allows to efficiently approximate the solution $V[\overline{m}]$ to~\eqref{eq:hjb_m}. The obtained approximation of the feedback law~\eqref{def:approximated_feedback} can then be used to approximate the best response ${\rm BR}[\overline{m}]$ by solving the continuity equation~\eqref{eq:continuity_eq_depending_on_m} via a NN method explained below. Combining both approximations with the fictitious play iterates~\eqref{eq:fictitious_play_method}, we obtain a ML method to approximate solutions to high-dimensional first-order MFG systems. 

{\it  Initialisation.} To initialize the iterative procedure, we compute a starting pair $(\overline{m}, V[\overline{m}])$ by solving the coupled MFG system via a joint training of the value function and the flow map. We introduce a value network $V_{\theta}^{\text{NN}}$ and a generator network $\Phi_{\omega}^{\text{NN}}$. These networks are trained simultaneously by minimizing a loss function $\mathcal{L}(\theta, \omega) = \mathcal{L}_{\text{HJB}}(\theta, \omega) + \mathcal{L}_{\text{ODE}}(\theta, \omega)$.
The term $\mathcal{L}_{\text{HJB}}$ penalizes the residual of the HJB equation~\eqref{eq:hjb_m} for $V_{\theta}^{\text{NN}}$, where the coupling term $F[\overline{m}](t,x)$ is approximated using the empirical distribution of particles $x_i(t) = \Phi_{\omega}^{\text{NN}}(t, x_0^i)$ generated by the flow map. This residual is evaluated on both the generated trajectories and a set of random points in the domain.
The term $\mathcal{L}_{\text{ODE}}$ enforces the consistency of the flow map with the dynamics derived from the value function, penalizing the difference between $\partial_t \Phi_{\omega}^{\text{NN}}$ and the drift field $b(\cdot, \cdot, \alpha_{\theta}^{\text{NN}})$, where $\alpha_{\theta}^{\text{NN}}$ is the feedback control associated with $V_{\theta}^{\text{NN}}$.
This pre-training step yields an initial mean field flow $\overline{m}$ and a corresponding value function approximation.

{\it  Neural network approximation.} Given a neural network approximation $V_{\theta^{*}}^{\text{NN}}[\overline{m}]$ of $V[\overline{m}]$, constructed with the IGT method, we train a neural network $\Phi_{\omega}^{\text{NN}}[\overline{m}]$, called {\it generator network}, to approximate the flow $\Phi[\overline{m}]$ defined by~\eqref{eq:optimal_dynamics_time_0}. Here, $\omega$ denotes a parameter belonging to a parameter space $\mathcal{W}$ and the architecture used to build each coordinate of $\Phi_{\omega}^{\text{NN}}[\overline{m}]$ is similar to the one used for $V_{\theta}^{\text{NN}}[\overline{m}]$, employing a smooth transition function to ensure the equality $\Phi_{\omega}^{\text{NN}}[\overline{m}](0,x)=x$ for all $x\in\RR^{d}$. In our numerical implementations in Section~\ref{subsec:numerical_mfg}, the architecture that we consider has three hidden layers, constant connection weights $\beta_i=0.5$, and constant activation function $\sigma_{i}$ given by the ReLu function.

{\it  Data generation.} In order to train the generator network we first consider a sample of initial states $\{x_{0}^{i}\}_{i=1}^{S}$ drawn from $m_0$ and, for every $i=1,\hdots,S$, we use an IVP solver to generate an approximation of the solution to 
\begin{equation}
\left\{\begin{aligned}
\dot{\x}(s) &=b(s,\x(s),\alpha_{\theta^*}(s,\x(s)))\\
&=-\partial_{p}\H\left(s,\x(s),\nabla_{x}V_{\theta^*}^{\text{NN}}[\overline{m}](s,\x(s))\right)\quad\text{for a.e. }s\in [0,T],\\
\x(0)&=x_{0}^{i},
\end{aligned}
\right.
\label{eq:optimal_dynamics_time_0_with_VNN}
\end{equation}
where we recall that $\alpha_{\theta^*}$ is defined by~\eqref{eq:feedback_control_NN}. This yields  a sequence of times $t_{1}^{i},\hdots,t_{N^{i}}^{i}$ and states $x_{1}^{i},\hdots,x_{N^{i}}^{i}$, ensuring a required level of precision, producing the data set 
\begin{equation}
\mathcal{D}_{\text{MFG}}=\left\{\left(t_{j}^{i},x_{j}^{i}\right)\,|\,i=0,\hdots,S, \, j=1,\dots,N^{i}\right\}.
\label{eq:data_G}
\end{equation} 

{\it  Model training.} The neural network $\Phi_{\omega}^{\text{NN}}[\overline{m}]$ is trained by penalizing deviations from the data set $\mathcal{D}_{\text{MFG}}$ and the residual of the ODE dynamics~\eqref{eq:Phi_EDP}, with $V[\overline{m}]$ replaced by $V^{\text{NN}}_{\theta^*}[\overline{m}]$, over points $\{(t^m,x_{0}^{m})\}_{m=1}^M$, where $\{t_{m}\}_{m=1}^{M}$ and $\{x_{0}^{m}\}_{m=1}^{M}$ are independent samples from the uniform distribution in $[0,T]$ and $m_0$, respectively. More precisely, to update $\omega$ one considers the following optimization problem \begin{equation} 
\min_{\omega\in\mathcal{W}}\;\mathrm{Loss}_{\mathrm{gen}}(\omega),
\label{eq:minimization_complete_loss_generator}
\end{equation} 
where 
\begin{equation}
\mathrm{Loss}_{\mathrm{gen}}(\omega):= \lossg(\omega)+\lambda_{2}\lossODE(\omega)\quad\text{for all }\omega\in\mathcal{W},
\label{eq:loss_total_gen}
\end{equation}
with $\lambda_{2}>0$ and, for every $\omega\in\mathcal{W}$,
\begin{equation}
\begin{aligned}
&\lossg(\omega):=\frac{1}{S} \sum_{i=1}^S\frac{1}{N_{i}}\sum_{j=1}^{N_{i}}\left\|x_j^i-\Phi_{\omega}^{\mathrm{NN}}[\overline{m}]\left(t_j^i,x_0^i \right)\right\|^2,\\
&\lossODE(\omega):=\frac{1}{M} \sum_{m=1}^{M}\Big\|\partial_{t} \Phi_{\omega}^{\mathrm{NN}}[\overline{m}](t_{m},x_{0}^{m})- b\left(t_m,\Phi^{\mathrm{NN}}_{\omega}[\overline{m}](t_{m},x_{0}^{m}),\alpha_{\theta^*}(t_m, \Phi_{\omega}^{\mathrm{NN}}[\overline{m}](t_{m},x_{0}^{m}))\right)\Big\|^2.
\end{aligned}
\label{eq:losses_generator}
\end{equation}
The last update of $\omega$ after convergence is achieved is denoted as $\omega^*$. 
\medskip

{\it  Update and error measures.} Given the parameter $\omega^*\in\mathcal{W}$ obtained in the previous step, we consider a new batch $\B=\{x_{0}^{b}\}_{b=1}^{B}$ of initial conditions sampled from $m_{0}$ and approximate $\text{BR}[\overline{m}]$ by the following curve of empirical measures (see \eqref{eq:push_forward_initial_measure})
\begin{equation}
[0,T]\ni t\mapsto \overline{\mu}(t):=\frac{1}{B}\sum_{b=1}^{B}\delta_{\Phi_{\omega^*}^{\text{NN}}[\overline{m}](t,x_{0}^{b})}\in\mathcal{P}(\RR^{d}).
\label{eq:mu_FP}
\end{equation}
Assuming that $\overline{m}$ has been already updated $k$ times, the $k+1$ update is defined through the fictitious play iteration~\eqref{eq:fictitious_play_method}:
\begin{equation}
(\forall\,t\in[0,T])\quad \overline{m}(t)\leftarrow \overline{m}(t)+\frac{1}{k+1}(\overline{\mu}(t)-\overline{m}(t)).
\label{eq:fictitious_play_method_numerical}
\end{equation}
In our numerical experiments, we adopt a stopping criterion based on the proximity between the distributions $\overline{m}$ and $\overline{\mu}$ evaluated over a discrete time grid $\T \subset [0,T]$. To quantify this proximity, at each time $t\in\T$ we employ the Sinkhorn divergence $S_{\varepsilon}(\overline{m}(t),\overline{\mu}(t))$, as introduced in~\cite{genevay2018learning}, where $\varepsilon > 0$ denotes a regularization parameter. This divergence offers a computational advantage over classical optimal transport, as it can be evaluated more efficiently and exhibits significantly lower computational complexity, particularly in high-dimensional cases~\cite{genevay2019sample}. Specifically, we consider the following aggregated metric:
\begin{equation}
{\bf S_{\varepsilon}^{\infty}}(\overline{m},\overline{\mu}) := \max_{t\in\T} S_{\varepsilon}(\overline{m}(t), \overline{\mu}(t)),
\label{eq:w_metrics}
\end{equation}
which is computed using the \texttt{geomloss} library~\cite{feydy2019interpolating}.  

We also monitor the convergence to $0$ of the following approximation of the exploitability $\psi(\alpha_{\theta^*})$ (see~\eqref{eq:exploitability_continuous}):
\begin{equation}
\psi^{\B}(\alpha_{\theta^*})=\frac{1}{B}\sum_{b=1}^{B}\left(\tilde{J}[\overline{\mu}](x_{0}^{b},\alpha_{\theta^*})-V^{\text{NN}}_{\theta^*}[\overline{\mu}](0,x_{0}^{b})\right),
\label{eq:exploitability_numerical}
\end{equation}
where, for a given uniform time grid $\{t_{k}\}_{k=0}^{N}\subset[0,T]$ with time step $\Delta t$, 
$$
\tilde{J}[\overline{\mu}](x_{0}^{b},\alpha_{\theta^*}):=\Delta t\sum_{k=0}^{N-1}\ell\left(t_k,\Phi_{\omega^*}^{\mathrm{NN}}[\overline{m}](t_{k},x_{0}^{b}),\alpha_{\theta^*}(t_k,\Phi_{\omega^*}^{\mathrm{NN}}[\overline{m}](t_{k},x_{0}^{b})),\overline{\mu}(t_k)\right)+G\left(\Phi_{\omega^*}^{\mathrm{NN}}[\overline{m}](T,x_{0}^{b}),\overline{\mu}(T)\right).
$$

From equation \eqref{eq:fictitious_play_method_numerical}, it follows that computing $\overline{m}$ at iteration $k$ of the fictitious play method requires using all the generators computed up to that iteration.
Since storing and managing this growing collection of generators becomes increasingly expensive, to keep the method computationally feasible, we impose a maximum number of iterations $K_{{\rm max}}$ in the fictitious play procedure.
We summarize in Algorithm~\ref{alg: IGT-MFG} the proposed procedure to approximate MFG equilibria. 

\begin{algorithm}[hbt!]
\begin{algorithmic}
\Require Initial guess $[0,T]\ni t\mapsto \overline{m}(t)\in\P(\RR^d)$, batch sizes $M_{1},\,M_{2},\,S_{1},\,S_{2}$, number of rounds $R$, and maximum number of fictitious play iterations $K_{\rm{max}}$.
\Require Batch of initial conditions $\B=\{x_{0}^{b}\}_{b=1}^{B}$ sampled from $m_0$ and  tolerance parameter $\texttt{tol}>0$. 
\Require Initialize neural network parameters $\theta^{*}\in\Theta$ and $\omega^{*}\in\W$. 
\Ensure $\delta_0\gets\texttt{tol}+1$,  $\overline{m}_0(t)\gets \overline{m}(t)$ for all $t\in[0,T]$.
\smallskip
\For{$k = 0$ \textbf{to} $K_{\max}-1$} 
    \If{$\delta_{k}\le\mathtt{tol}$} \textbf{break} \EndIf
    \State Update $\theta^*$ by approximating $V[\overline{m}_{k}]$ using the IGT method with input parameters $M_{1},\,S_1,\,R$.  
    \State {\it Data generation}
    \State \quad \quad  Generate $\mathcal{D}_{\text{MFG}}$, given by~\eqref{eq:data_G},  using a batch $\{x_{0}^{i}\}_{i=1}^{S_{2}}$, sampled from $m_0$, and $\nabla_{x}V^{\mathrm{NN}}_{\theta^{*}}[\overline{m}^k]$.
    \State {\it Training of $\Phi^{\rm NN}_{\omega}[\overline{m}_k]$}
    \While{not converged} 
        \State \quad \quad Sample a batch $\{x_{0}^{m}\}_{m=1}^{M_{2}}$ from  $m_{0}$ and $\{t_{m}\}_{m=1}^{M_{2}}$ from a uniform distribution in $[0,T]$.
        \State \quad \quad Compute $\text{Loss}_{\text{gen}}$, using $\mathcal{D}_{\text{MFG}}$ for $\text{Loss}_{\Phi}$ and the batch $\{(t_{m},x_{0}^{m})\}_{m=1}^{M_{2}}$ for $\lossODE$. 
        \State \quad \quad Backpropagate $\text{Loss}_{\text{gen}}$ to update $\omega^{*}$.
    \EndWhile
    \State {\it Update}
    \State \quad \quad  Compute $\mu_{k+1}$ using \eqref{eq:mu_FP} with the batch $\B$ and $\Phi^{\rm NN}_{\omega^*}[\overline{m}_{k}]$.
    \State \quad \quad  Update $\delta_{k+1} \gets {\bf S_{\epsilon}^{\infty}}(\overline{m}_{k},\mu_{k+1})$.
    \State \quad \quad  Update $\overline{m}_{k+1}\gets\overline{m}_{k}+\frac{1}{k+1}(\mu_{k+1}-\overline{m}_{k})$.
\EndFor
\State \Return $\theta^{*}$ and $\omega^{*}$.
\end{algorithmic}
\caption{IGT-MFG}
\label{alg: IGT-MFG}
\end{algorithm}

\subsubsection{{\bf Evaluating IGT-MFG}}
{\it One  dimensional test.} In this test, we consider the case $d=1$ and we initialize the IGT-MFG method by performing a joint training of the value function and the flow map, using a batch of size $256$ sampled from the initial distribution $m_0$ for the ODE loss and a batch of size $256$ sampled uniformly from the domain $[-2,2]$ for the HJB residual. At each iteration $k$ of the fictitious play method, we update $\theta^*$ using the IGT algorithm (see Algorithm~\ref{alg:IGT}). In the initialization step of the IGT algorithm, we consider a batch of uniformly distributed points in the time-space domain $[0,1] \times [-2,2]$ of size $M_1=256$. In the data generation step,  we take a batch of size $S_1=32$  of initial conditions sampled from $m_0$. In the training step, the computation of the penalization term $\lossHJB$ in~\eqref{eq:loss_total} is performed using the same size $M_1$ of uniformly distributed points in $[0,1] \times [-2,2]$ and penalization parameter $\lambda_1 = 1$.  Then to compute $\Phi^{\rm NN}_{\omega^*}$ we first use a batch of size $S_2=256$, sampled from $m_0$, to generate data. In the training step, we use a batch of size $M_2=256$ to compute $\lossODE$ and we take $\lambda_2 = 0.5$ in the definition of~$\mathrm{Loss}_{\mathrm{gen}}$ (see~\eqref{eq:loss_total_gen}).

{\it High-dimensional tests.} We now consider the same experiment in higher dimensions $d \in \{10, 50, 100\}$. We initialize the IGT-MFG method using batches of size $512$ for both the initial distribution and the domain. Throughout the algorithm, we set the batch sizes $M_1$, $M_2$, and $S_2$ to $512$. For the data generation step in the IGT method, we set $S_1=128$ for $d=10$, and increase it to $S_1=512$ for $d=50$ and $d=100$. Regarding the penalization parameter in $\mathrm{Loss}_{\mathrm{gen}}$, we choose $\lambda_2 = 0.5$ for $d=10$, $\lambda_2 = 0.1$ for $d=50$, and $\lambda_2 = 0.01$ for $d=100$. All uniform samplings are taken in the time-space domain $[0,1] \times [-2,2]^d$.  Table~\ref{tab: MFG10} and Figure~\ref{fig: MFG10} summarize the performance and convergence behavior of the algorithm after $Q=5$ fictitious play cycles.  We observe that we achieve an error of order $10^{-4}$ for ${\bf S_{\varepsilon}^{\infty}}(\overline{m}^k,\mu_{k+1})$ at cycle 3, which remain stable for the next two cycles.


{\it Two  dimensional test.} We consider the case $d=2$ and we initialize the IGT-MFG method by performing a joint training of the value function and the flow map, using a batch of size $256$ sampled from the initial distribution $m_0$ for the ODE loss and a batch of size $256$ sampled uniformly from the domain $[-2,2]^2$ for the HJB residual. At each iteration $k$ of the fictitious play method, we update $\theta^*$ using the IGT algorithm (see Algorithm~\ref{alg:IGT}). The DGM initialization employs a batch of $M_{1} = 256$ uniformly distributed points in the time-space domain $[0,1] \times [-2,2]^2$. We then sample a batch of $S_{1} = 32$ initial conditions from $m_{0}$ to form the dataset $\mathcal{D}_{\mathrm{OC}}$. The penalization term $\lossHJB$ in~\eqref{eq:loss_total} is evaluated with the same size $M_1$ of points uniformly distributed in the same domain and weight $\lambda_{1} = 1$. To compute the generator $\Phi^{\mathrm{NN}}_{\omega^{\ast}}$ we use a batch of size $S_{2} = 256$ from $m_{0}$ to build $\mathcal{D}_{\mathrm{MFG}}$, together with $M_{2} = 256$ points for the penalization term $\lossODE$ in~\eqref{eq:loss_total_gen}, employing the weight $\lambda_{2} = 0.5$. On the left of Figure~\ref{fig: MFG_obs} we see the approximated optimal trajectories. Table~\ref{tab: obs_2} provides the exploitability and the aggregated Sinkhorn divergences across the iterations.

{\it High-dimensional test.} We now consider the same experiment in a higher dimension $d=10$. Given the increased complexity of high-dimensional settings, we double the batch sizes to $M_1 = 512$, $S_1 = 64$, $S_2 = 512$, and $M_2 = 512$. Furthermore, we set the penalization parameters to $\lambda_1=0.5$ and $\lambda_2=0.001$. 